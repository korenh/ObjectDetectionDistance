{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a284a905-f451-49ec-9339-031a12b23d0d",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bdccd-93ec-4f33-85e1-7d73174ab223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import IPython\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, NoReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf698434-476d-4828-9acf-84ce3833fea8",
   "metadata": {},
   "source": [
    "### Params\n",
    "\n",
    "- STREAM_INPUT - Video streaming source\n",
    "- KNOWN_DISTANCE - Known distance from the reference image *Inches\n",
    "- PERSON_WIDTH - Person avg width in real life *Inches\n",
    "- CAR_WIDTH - Car width in real life *Inches\n",
    "- BLACK_RGB - RGB Value for black\n",
    "- FIG_SIZE - Streaming figsize view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee1f8c-efc6-45d0-91c6-824f9d0ebb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_INPUT = './resource/sample.mp4'\n",
    "KNOWN_DISTANCE = 120.0\n",
    "PERSON_WIDTH = 16.0\n",
    "CAR_WIDTH = 100.0\n",
    "INCH_TO_METER = 0.0254\n",
    "BLACK_RGB = (0, 0, 0)\n",
    "FIG_SIZE = (12,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d692e-9f58-4770-8c0a-9cb70a1c1573",
   "metadata": {},
   "source": [
    "### NetParams\n",
    "\n",
    "- Currently used pretrined YoloV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da025c-6fbe-4feb-8513-3992e0954b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93bcdd-bb62-4633-8baf-25be6a36047a",
   "metadata": {},
   "source": [
    "### Distance measurement\n",
    "\n",
    "- focal_distance - ( reference_width * known distance from the object ) / real object width \n",
    "- distance_finder - ( focal_distance * real object width ) / object width in frame \n",
    "\n",
    "### Object detection\n",
    "\n",
    "- predict - predict objects & draw rectangle\n",
    "\n",
    "### Preview\n",
    "\n",
    "- display - display video streaming live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed491422-5b34-4eb9-8c53-3ce9d827b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image: np.ndarray, calc_distance: bool = True) -> List[Dict]:\n",
    "    detections =[]\n",
    "    for res in model(image).pandas().xyxy:\n",
    "        for i, d in res.iterrows():\n",
    "            x, y, w, h = int(d['xmin']) , int(d['ymin']) , int(d['xmax'] - d['xmin']), int(d['ymax']- d['ymin'])\n",
    "            obj = {'cls': d['name'] , 'bbox': [x, y, w, h]}\n",
    "            obj['distance'] = distance(obj) if calc_distance else ''\n",
    "            detections.append(obj)        \n",
    "        \n",
    "    return detections\n",
    "\n",
    "\n",
    "def focal_distance(measured_distance: float, real_width: float, width_in_rf: float) -> float:\n",
    "    return (measured_distance * width_in_rf) / real_width\n",
    "\n",
    "\n",
    "def distance_finder(focal_length : float, real_object_width: float, width_in_frmae: float) -> float:\n",
    "    return (focal_length * real_object_width) / width_in_frmae\n",
    "\n",
    "\n",
    "def distance(detect: Dict) -> str:\n",
    "    return {'person': round(distance_finder(focal_person, PERSON_WIDTH, detect['bbox'][2]) * INCH_TO_METER, 1),\n",
    "            'car': round(distance_finder(focal_car, CAR_WIDTH, detect['bbox'][2]) * INCH_TO_METER, 1)}.get(detect['cls'], '')\n",
    "                           \n",
    "def display(frame: np.ndarray, detections: List[Dict]) -> NoReturn:\n",
    "    for d in detections:\n",
    "        cv2.rectangle(frame, d['bbox'], BLACK_RGB, 1)\n",
    "        cv2.putText(frame, '{} {}'.format(d['distance'], d['cls']) , (d['bbox'][0], d['bbox'][1]- 20), cv2.FONT_HERSHEY_COMPLEX, .7, BLACK_RGB, 2)\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    plt.imshow(frame)\n",
    "    f = BytesIO()\n",
    "    plt.savefig(f, format='jpeg'), plt.close()\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "    IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075cb91-f48f-49f8-9177-25377977da95",
   "metadata": {},
   "source": [
    "### Reference objects\n",
    "\n",
    "- Used to determine detected object size to the real world object\n",
    "- Currently calculated for person & cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd475f-9b5d-4127-a7d0-df14c1e6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_person = cv2.imread('./resource/ref_person.jpg')\n",
    "ref_car = cv2.imread('./resource/ref_car.jpg')\n",
    "\n",
    "person_data = predict(ref_person, False)\n",
    "car_data = predict(ref_car, False)\n",
    "\n",
    "focal_person = focal_distance(KNOWN_DISTANCE, PERSON_WIDTH, person_data[0]['bbox'][2])\n",
    "focal_car = focal_distance(KNOWN_DISTANCE, CAR_WIDTH, car_data[0]['bbox'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499867ea-38b2-415b-bdaf-3f4f77fc7ce0",
   "metadata": {},
   "source": [
    "### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e31a24-3daa-4b54-9372-8c20b6d569b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(STREAM_INPUT)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    detections = predict(frame)\n",
    "    display(frame, detections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
