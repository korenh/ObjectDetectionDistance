{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a284a905-f451-49ec-9339-031a12b23d0d",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536bdccd-93ec-4f33-85e1-7d73174ab223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import IPython\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, NoReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf698434-476d-4828-9acf-84ce3833fea8",
   "metadata": {},
   "source": [
    "### Params\n",
    "\n",
    "- KNOWN_DISTANCE - Known distance from the reference image *Inches\n",
    "- PERSON_WIDTH - Person avg width in real life *Inches\n",
    "- CAR_WIDTH - Car width in real life *Inches\n",
    "- CONFIDENCE_THRESHOLD - Minimal threshold to detect object \n",
    "- NMS_THRESHOLD - Minimal NMS to suppress the less likely bounding boxes by score & IOU\n",
    "- NET_IMG_SIZE - Preferable image size for net\n",
    "- BLACK_RGB - RGB Value for black\n",
    "- FIG_SIZE - Streaming figsize view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aee1f8c-efc6-45d0-91c6-824f9d0ebb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_DISTANCE = 100.0\n",
    "PERSON_WIDTH = 16.0\n",
    "CAR_WIDTH = 100.0\n",
    "CONFIDENCE_THRESHOLD = 0.4\n",
    "NMS_THRESHOLD = 0.3\n",
    "NET_IMG_SIZE = (512, 512)\n",
    "INCH_TO_METER = 0.0254\n",
    "BLACK_RGB = (0, 0, 0)\n",
    "FIG_SIZE = (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d692e-9f58-4770-8c0a-9cb70a1c1573",
   "metadata": {},
   "source": [
    "### NetParams\n",
    "\n",
    "- Currently used pretrined YoloV4 weights with full classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8da025c-6fbe-4feb-8513-3992e0954b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoloNet = cv2.dnn.readNet('./resource/yolov4-tiny.weights', './resource/yolov4-tiny.cfg')\n",
    "yoloNet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "yoloNet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "model = cv2.dnn_DetectionModel(yoloNet)\n",
    "model.setInputParams(size=NET_IMG_SIZE, scale=1/255, swapRB=True)\n",
    "\n",
    "with open(\"./resource/classes.txt\", \"r\") as f:\n",
    "    names = [name.strip() for name in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93bcdd-bb62-4633-8baf-25be6a36047a",
   "metadata": {},
   "source": [
    "### Distance measurement\n",
    "\n",
    "- focal_distance - ( reference_width * known distance from the object ) / real object width\n",
    "- distance_finder - ( focal_distance * real object width ) / object width in frame \n",
    "\n",
    "### Object detection\n",
    "\n",
    "- predict - predict objects & draw rectangle\n",
    "\n",
    "### Preview\n",
    "\n",
    "- display - display video streaming live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed491422-5b34-4eb9-8c53-3ce9d827b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image: np.ndarray) -> List[list]:\n",
    "    data_list =[]\n",
    "    classes, scores, boxes = model.detect(image, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "\n",
    "    for (classid, score, box) in zip(classes, scores, boxes):    \n",
    "        \n",
    "        cv2.rectangle(image, box, BLACK_RGB, 2)\n",
    "        cv2.putText(image, str(names[classid]), (box[0], box[1]-14), cv2.FONT_HERSHEY_COMPLEX, 1, BLACK_RGB, 2)\n",
    "    \n",
    "        if classid == 0: # person\n",
    "            data_list.append([names[classid], box[2], (box[0], box[1]-2)])\n",
    "            \n",
    "        if classid == 2: # car  \n",
    "            data_list.append([names[classid], box[2], (box[0], box[1]-2)])\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def focal_distance(measured_distance: float, real_width: float, width_in_rf: float) -> float:\n",
    "    return (measured_distance * width_in_rf) / real_width\n",
    "\n",
    "\n",
    "def distance_finder(focal_length : float, real_object_width: float, width_in_frmae: float) -> float:\n",
    "    return (focal_length * real_object_width) / width_in_frmae\n",
    "\n",
    "\n",
    "def display(frame: np.ndarray) -> NoReturn:\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    plt.imshow(frame)\n",
    "    f = BytesIO()\n",
    "    plt.savefig(f, format='jpeg'), plt.close()\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075cb91-f48f-49f8-9177-25377977da95",
   "metadata": {},
   "source": [
    "### Reference params\n",
    "\n",
    "- Used to determine detected object size to the real world object\n",
    "- Currently calculated for person & cars/trucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dabd475f-9b5d-4127-a7d0-df14c1e6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_person = cv2.imread('./images/sample_person.png')\n",
    "ref_car = cv2.imread('./images/sample_car.png')\n",
    "\n",
    "person_data = predict(ref_person)\n",
    "person_width_in_rf = person_data[0][1]\n",
    "focal_person = focal_distance(KNOWN_DISTANCE, PERSON_WIDTH, person_width_in_rf)\n",
    "\n",
    "car_data = predict(ref_car)\n",
    "car_width_in_rf = car_data[0][1]\n",
    "focal_car = focal_distance(KNOWN_DISTANCE, CAR_WIDTH, car_width_in_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499867ea-38b2-415b-bdaf-3f4f77fc7ce0",
   "metadata": {},
   "source": [
    "### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e31a24-3daa-4b54-9372-8c20b6d569b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./images/sample1.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    data, distance = predict(frame), 0\n",
    "    \n",
    "    for d in data:\n",
    "        if d[0] =='person':\n",
    "            distance = distance_finder(focal_person, PERSON_WIDTH, d[1]) * INCH_TO_METER\n",
    "\n",
    "        if d[0] == 'car':\n",
    "            distance = distance_finder(focal_car, CAR_WIDTH, d[1]) * INCH_TO_METER\n",
    "\n",
    "        x, y = d[2]\n",
    "        cv2.putText(frame, str(round(distance,1)), (x+5,y+13), cv2.FONT_HERSHEY_COMPLEX, 0.8, BLACK_RGB , 2)\n",
    "\n",
    "    display(frame)\n",
    "    IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b35db-9f0e-43ff-bfa6-e6908e6eecfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
